{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting with a Simple Example: Digits Data from sklearn\n",
    "Let's start our adventure into convolutional networks with a simple example of the digits dataset. This was not always such a simple example, but modern day computing power and open source tools has made it a significantly more tractable problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1437, 1, 8, 8)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(2)\n",
    "np.random.seed(0) # using this to help make results reproducible\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "X = digits.data.astype(np.float32)/16.0 - 0.5\n",
    "y = digits.target.astype(np.int32)\n",
    "\n",
    "X = X.reshape((X.shape[0],1,8,8)) # reshape as images\n",
    "\n",
    "# Split it into train / test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Split X_train again to create validation data\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train,y_train,test_size=0.2)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 734.4x496.8 with 18 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# What do these images look like?\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# a helper plotting function\n",
    "def plot_gallery(images, titles, h, w, n_row=3, n_col=6):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.7 * n_col, 2.3 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].squeeze(), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "\n",
    "plot_gallery(X_train, y_train, 8, 8) # defaults to showing a 3 by 6 subset of the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "____\n",
    "# A very Simple ConvNet Versus a Raw Pixel Input MLP\n",
    "Wouldn't it be great if we did not need to specify the correct filters (like in DAISY)? What if we could just let the weights of the convolution be found through neural network training methods. Of course, we can! Let's do one example with a simple CNN architecture and compare it to the performance of a pixel wise MLP.\n",
    "\n",
    "In general, the flattened images placed through a MLP can be quite accurate (as we have seen in the past). Even so, using convolitional filters and pooling should provide us with some better resilience to small perturbations in the images. \n",
    "\n",
    "**BONUS**: Let's start using the sequential API, rather than the funcitonal API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1437, 1, 8, 8)\n",
      "(360, 1, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 10\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since this is not binary, we should go ahead and one-hot encode the inputs\n",
    "y_train_ohe = keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test_ohe = keras.utils.to_categorical(y_test, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23c1664feb0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# make a 3 layer keras MLP\n",
    "mlp = Sequential()\n",
    "mlp.add( Flatten() ) # make images flat for the MLP input\n",
    "mlp.add( Dense(input_dim=X_train.shape[1], units=30, \n",
    "               activation='relu') )\n",
    "mlp.add( Dense(units=15, activation='relu') )\n",
    "mlp.add( Dense(NUM_CLASSES) )\n",
    "mlp.add( Activation('softmax') )\n",
    "\n",
    "mlp.compile(loss='mean_squared_error',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "mlp.fit(X_train, y_train_ohe, \n",
    "        batch_size=32, epochs=150, \n",
    "        shuffle=True, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 16, 8, 8)          80        \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 4, 4)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " activation (Activation)     (None, 16, 4, 4)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                2570      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,650\n",
      "Trainable params: 2,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Wall time: 762 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# make a CNN with conv layer and max pooling\n",
    "cnn = Sequential() \n",
    "cnn.add( Conv2D(filters=16, kernel_size= (2, 2), padding='same', \n",
    "                input_shape=(1,8,8),\n",
    "               data_format=\"channels_first\") )  # this specifies where the channels are in the shape of X_train\n",
    "\n",
    "cnn.add( MaxPooling2D(pool_size=(2, 2), \n",
    "                      data_format=\"channels_first\") )\n",
    "cnn.add( Activation('relu') )\n",
    "# add one layer on flattened output\n",
    "cnn.add( Flatten() )\n",
    "cnn.add( Dense(NUM_CLASSES) )\n",
    "cnn.add( Activation('softmax') )\n",
    "\n",
    "cnn.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1437, 1, 8, 8)\n",
      "Epoch 1/150\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "print(X_train.shape)\n",
    "# Let's train the model \n",
    "cnn.compile(loss='mean_squared_error',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "cnn.fit(X_train, y_train_ohe, \n",
    "        batch_size=32, epochs=150, \n",
    "        shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as mt\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "def compare_mlp_cnn(cnn, mlp, X_test, y_test, labels='auto'):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    if cnn is not None:\n",
    "        yhat_cnn = np.argmax(cnn.predict(X_test), axis=1)\n",
    "        acc_cnn = mt.accuracy_score(y_test,yhat_cnn)\n",
    "        plt.subplot(1,2,1)\n",
    "        cm = mt.confusion_matrix(y_test,yhat_cnn)\n",
    "        cm = cm/np.sum(cm,axis=1)[:,np.newaxis]\n",
    "        sns.heatmap(cm, annot=True, fmt='.2f',xticklabels=labels,yticklabels=labels)\n",
    "        plt.title('CNN: '+str(acc_cnn))\n",
    "    \n",
    "    if mlp is not None:\n",
    "        yhat_mlp = np.argmax(mlp.predict(X_test), axis=1)\n",
    "        acc_mlp = mt.accuracy_score(y_test,yhat_mlp)\n",
    "        plt.subplot(1,2,2)\n",
    "        cm = mt.confusion_matrix(y_test,yhat_mlp)\n",
    "        cm = cm/np.sum(cm,axis=1)[:,np.newaxis]\n",
    "        sns.heatmap(cm,annot=True, fmt='.2f',xticklabels=labels,yticklabels=labels)\n",
    "        plt.title('MLP: '+str(acc_mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_mlp_cnn(cnn,mlp,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# changes: \n",
    "#    1. increased kernel size\n",
    "cnn2 = Sequential()\n",
    "cnn2.add( Conv2D(filters=16, kernel_size= (3, 3), \n",
    "                padding='same', input_shape=(1,8,8),\n",
    "                data_format=\"channels_first\") )\n",
    "cnn2.add( Activation('relu') )\n",
    "cnn2.add( MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\") )\n",
    "# add one layer on flattened output\n",
    "cnn2.add( Flatten() )\n",
    "cnn2.add( Dense(NUM_CLASSES, activation='softmax') )\n",
    "\n",
    "# Let's train the model \n",
    "cnn2.compile(loss='mean_squared_error',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "cnn2.fit(X_train, y_train_ohe, \n",
    "        batch_size=32, epochs=150, \n",
    "        shuffle=True, verbose=0)\n",
    "\n",
    "compare_mlp_cnn(cnn2,mlp,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# changes: \n",
    "#    1. increased kernel size\n",
    "#    2. add another conv/pool layer \n",
    "cnn3 = Sequential()\n",
    "\n",
    "num_filt_layers = [32, 32]\n",
    "for num_filters in num_filt_layers:\n",
    "    cnn3.add( Conv2D(filters=num_filters, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding='same',\n",
    "                     data_format=\"channels_first\") )\n",
    "    cnn3.add( Activation('relu'))\n",
    "    cnn3.add( MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\") )\n",
    "    \n",
    "\n",
    "# add one layer on flattened output\n",
    "cnn3.add( Flatten() )\n",
    "cnn3.add( Dense(NUM_CLASSES) )\n",
    "cnn3.add( Activation('softmax') )\n",
    "\n",
    "# Let's train the model \n",
    "cnn3.compile(loss='mean_squared_error',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "cnn3.fit(X_train, y_train_ohe, \n",
    "        batch_size=32, epochs=150, \n",
    "        shuffle=True, verbose=0)\n",
    "\n",
    "compare_mlp_cnn(cnn3,mlp,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# changes: \n",
    "#    1. increased kernel size\n",
    "#    2. add another conv/pool layer with increasing num filters\n",
    "#    3. add more layers once flattened\n",
    "cnn4 = Sequential()\n",
    "\n",
    "num_filt_layers = [24, 48]\n",
    "for num_filters in num_filt_layers:\n",
    "    cnn4.add( Conv2D(filters=num_filters, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding='same',data_format=\"channels_first\"))\n",
    "    cnn4.add( Activation('relu'))\n",
    "    cnn4.add( MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\"))\n",
    "    \n",
    "\n",
    "# add one layer on flattened output\n",
    "cnn4.add( Flatten() )\n",
    "cnn4.add( Dense(100) )\n",
    "cnn4.add( Activation('relu') )\n",
    "cnn4.add( Dense(NUM_CLASSES) )\n",
    "cnn4.add( Activation('softmax') )\n",
    "\n",
    "# Let's train the model \n",
    "cnn4.compile(loss='mean_squared_error',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "cnn4.fit(X_train, y_train_ohe, \n",
    "        batch_size=32, epochs=150, \n",
    "        shuffle=True, verbose=0)\n",
    "\n",
    "compare_mlp_cnn(cnn4,mlp,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion MNIST\n",
    "Okay, so we are honing in on the best performance for the digits data. But, we really need something a bit bigger to work on. Enter: Fashion MNIST. We have already used this dataset in the past, where we got about 84% accuracy with our custom MLP. Lets see how well Keras and tensorflow can perform!\n",
    "\n",
    "Now let's load in a more diverse, harder to classify dataset: Fashion MNIST\n",
    "https://www.kaggle.com/zalando-research/fashionmnist\n",
    "\n",
    "**Labels**\n",
    "\n",
    "Each training and test example is assigned to one of the following labels:\n",
    "\n",
    "- 0 T-shirt/top\n",
    "- 1 Trouser\n",
    "- 2 Pullover\n",
    "- 3 Dress\n",
    "- 4 Coat\n",
    "- 5 Sandal\n",
    "- 6 Shirt\n",
    "- 7 Sneaker\n",
    "- 8 Bag\n",
    "- 9 Ankle boot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more data for handwriting recognition?\n",
    "# Let's use Raschka's implementation for using the mnist dataset:\n",
    "# https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    " \n",
    "def load_mnist(path, kind='fashion_train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path, '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, '%s-images-idx3-ubyte' % kind)\n",
    "        \n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)\n",
    " \n",
    "    return images, labels\n",
    "\n",
    "X_train, y_train = load_mnist('data/', kind='fashion_train')\n",
    "X_test, y_test = load_mnist('data/', kind='fashion_t10k')\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0],1,28,28)/255.0 - 0.5\n",
    "X_test = X_test.reshape(X_test.shape[0],1,28,28)/255.0 - 0.5\n",
    "\n",
    "print('X_train shape:', X_train.shape) \n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "\n",
    "# make one- hot encoded versions of the data\n",
    "y_train_ohe = keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test_ohe = keras.utils.to_categorical(y_test, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# make a keras MLP\n",
    "mlp = Sequential()\n",
    "mlp.add( Flatten() )\n",
    "mlp.add( Dense(input_dim=X_train.shape[1], units=100, activation='relu') )\n",
    "mlp.add( Dense(units=50, activation='relu') )\n",
    "mlp.add( Dense(units=50, activation='relu') )\n",
    "mlp.add( Dense(NUM_CLASSES) )\n",
    "mlp.add( Activation('softmax') )\n",
    "\n",
    "mlp.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "mlp.fit(X_train, y_train_ohe, \n",
    "        batch_size=32, epochs=15, \n",
    "        shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "compare_mlp_cnn(None,mlp,X_test,y_test,labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# changes: \n",
    "#    1. Baseline: 2 conv layers and two output layers\n",
    "cnn1 = Sequential()\n",
    "\n",
    "num_filt_layers = [24, 24]\n",
    "for num_filters in num_filt_layers:\n",
    "    cnn1.add( Conv2D(filters=num_filters, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding='same',data_format=\"channels_first\") )\n",
    "    cnn1.add( Activation('relu'))\n",
    "    cnn1.add( MaxPooling2D(pool_size=(2, 2), \n",
    "                           data_format=\"channels_first\") )\n",
    "    \n",
    "\n",
    "# add one layer on flattened output\n",
    "cnn1.add( Flatten() )\n",
    "cnn1.add( Dense(100, activation='relu') )\n",
    "cnn1.add( Dense(100, activation='relu') )\n",
    "cnn1.add( Dense(NUM_CLASSES, activation='softmax') )\n",
    "\n",
    "# Let's train the model \n",
    "cnn1.compile(loss='mean_squared_error',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# we need to exapnd the dimensions here to give the \n",
    "#   \"channels\" dimension expected by Keras\n",
    "cnn1.fit(X_train, y_train_ohe, \n",
    "        batch_size=32, epochs=15, \n",
    "        shuffle=True, verbose=1,\n",
    "        validation_data=(X_test,y_test_ohe))\n",
    "\n",
    "compare_mlp_cnn(cnn1,mlp,X_test,y_test,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Copy TensorFlow Architecture from \n",
    "#   Deep MNIST for experts\n",
    "#   https://www.tensorflow.org/versions/r0.11/tutorials/mnist/pros/index.html\n",
    "\n",
    "# Manipulated to mirror parts of this network:\n",
    "#   http://ankivil.com/mnist-database-and-simple-classification-networks/\n",
    "\n",
    "cnn2 = Sequential()\n",
    "\n",
    "num_filt_layers = [32, 64]\n",
    "for num_filters in num_filt_layers:\n",
    "    cnn2.add( Conv2D(filters=num_filters, \n",
    "                    kernel_size=(3,3), \n",
    "                    padding='same', \n",
    "                    activation='relu',\n",
    "                    data_format=\"channels_first\") ) # more compact syntax\n",
    "\n",
    "    # max pooling\n",
    "    cnn2.add( MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\") )\n",
    "    \n",
    "\n",
    "# add one layer on flattened output\n",
    "cnn2.add( Dropout(0.25) ) # add some dropout for regularization after conv layers\n",
    "cnn2.add( Flatten() )\n",
    "cnn2.add( Dense(1024, activation='relu') )\n",
    "cnn2.add( Dropout(0.5) ) # add some dropout for regularization, again!\n",
    "cnn2.add( Dense(NUM_CLASSES, activation='softmax') )\n",
    "\n",
    "# Let's train the model \n",
    "cnn2.compile(loss='categorical_crossentropy', # 'categorical_crossentropy' 'mean_squared_error'\n",
    "              optimizer='rmsprop', # 'adadelta' 'rmsprop'\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# we need to exapnd the dimensions here to give the \n",
    "#   \"channels\" dimension expected by Keras\n",
    "cnn2.fit(X_train, y_train_ohe, \n",
    "        batch_size=64, epochs=10, \n",
    "        shuffle=True, verbose=1,\n",
    "        validation_data=(X_test,y_test_ohe))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_mlp_cnn(cnn2,mlp,X_test,y_test,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn2.evaluate(X_test,y_test_ohe,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want to save this model for later?\n",
    "cnn2.save('large_data/mnist_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "cnn_mnist = load_model('large_data/mnist_cnn.h5')\n",
    "compare_mlp_cnn(cnn_mnist,None,X_test,y_test,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How can we make this train better with a deep network with more filters??\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
